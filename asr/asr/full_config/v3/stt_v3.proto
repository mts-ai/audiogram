syntax = "proto3";

import "google/protobuf/empty.proto";
import "response_header.proto";
import "stt_response.proto";

package mts.ai.audiogram.stt.v3;

message FileRecognizeRequest {
  RecognitionConfig config = 1;
  bytes audio = 2;
}

message RecognizeRequest {
  oneof streaming_request {
    StreamRecognitionConfig config = 1;
    bytes audio = 2;
  }
}

message RecognitionConfig {
  enum VoiceActivityMarkEventsMode {
    VA_DISABLE = 0;
    VA_ENABLE = 1;
    VA_ENABLE_ASYNC = 2;
  }
  AudioEncoding encoding = 1;
  uint32 sample_rate_hertz = 2;
  string language_code = 3;
  uint32 audio_channel_count = 4;
  bool split_by_channel = 5;
  string model = 6;
  bool enable_word_time_offsets = 7;
  VoiceActivityConfig va_config = 8;
  VoiceActivityMarkEventsMode va_response_mode = 9;
  GenderAgeEmotionConfig genderage_config = 10;
  AntiSpoofingConfig antispoofing_config = 11;
  ContextDictionaryConfig context_dictionary = 12;
  PunctuationConfig punctuation_config = 13;
  DenormalizationConfig denormalization_config = 14;
  SpeakerLabelingConfig speaker_labeling_config = 15;
}

message StreamRecognitionConfig {
  RecognitionConfig config = 1;
  bool single_utterance = 2;
  bool interim_results = 3;
}

enum AudioEncoding {
  ENCODING_UNSPECIFIED = 0;
  LINEAR_PCM = 1;
  FLAC = 2;
  MULAW = 3;
  ALAW = 20;
}

message VoiceActivityConfig {
  enum VoiceActivityDetectionAlgorithmUsage {
    USE_VAD = 0;
    DO_NOT_PERFORM_VOICE_ACTIVITY = 1;
    USE_DEP = 2;
    USE_ENHANCED_VAD = 3;
    USE_TARGET_SPEECH_VAD = 4;
  }

  VoiceActivityDetectionAlgorithmUsage usage = 1;
  oneof algo_options {
    VADOptions vad_options = 2;
    DEPOptions dep_options = 3;
    EnhancedVADOptions enhanced_vad_options = 4;
    TargetSpeechVADOptions target_speech_vad_options = 5;
  }
}

message VADOptions {
  enum VoiceActivityDetectionMode {
    VAD_MODE_DEFAULT = 0;
    SPLIT_BY_PAUSES = 1;
    ONLY_SPEECH = 2;
  }
  float threshold = 1;
  int32 speech_pad_ms = 2;
  uint32 min_silence_ms = 3;
  uint32 min_speech_ms = 4;
  VoiceActivityDetectionMode mode = 5;
}

message DEPOptions {
  float smoothed_window_threshold = 1;
  int32 smoothed_window_ms = 2;
}

message EnhancedVADOptions {
  int32 beginning_window_ms = 1;
  float beginning_threshold = 2;
  int32 ending_window_ms = 3;
  float ending_threshold = 4;
}

message TargetSpeechVADOptions {
  int32 beginning_window_ms = 1;
  float beginning_threshold = 2;
  int32 ending_window_ms = 3;
  float ending_threshold = 4;
}

message GenderAgeEmotionConfig {
  bool enable = 1;
}

message AntiSpoofingConfig {
  reserved 1;
  oneof false_rate_type {
    float FAR = 2;
    float FRR = 3;
  }
  uint32 max_duration_for_analysis_ms = 4;
  bool enable = 5;
}

message ContextDictionaryConfig{
  string dictionary_name = 1;
  float weight = 2;
}

message PunctuationConfig {
  bool enable = 1;
}

message DenormalizationConfig {
  bool enable = 1;
}

message SpeakerLabelingConfig{
  bool enable = 1;
  oneof speakers{
    uint32 max_speakers = 2;
    uint32 num_speakers = 3;
  }
}

message ModelsInfo {
  repeated ModelInfo models = 1;
  mts.ai.audiogram.response_header.v1.ResponseHeader header = 2;
}

message ModelInfo {
  string name = 1;
  uint32 sample_rate_hertz = 2;
  string language_code = 3;
  repeated string dictionary_name = 4;
}

service STT {
  rpc FileRecognize(FileRecognizeRequest) returns (mts.ai.audiogram.stt_response.v1.FileRecognizeResponse);
  rpc Recognize (stream RecognizeRequest) returns (stream mts.ai.audiogram.stt_response.v1.RecognizeResponse);
  rpc GetModelsInfo(google.protobuf.Empty) returns (ModelsInfo);
}